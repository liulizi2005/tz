# -*- coding: utf-8 -*-
"""
IMDb ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†æï¼ˆæŸ¥é‡å®‰å…¨ç‰ˆï¼‰
æ•°æ®é›†ï¼šhttps://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz
æ¨¡å‹ï¼šæ‰‹åŠ¨ TF-IDF + æ‰‹åŠ¨ SVM
ä¾èµ–ï¼šä»… Python æ ‡å‡†åº“ï¼ˆurllib, tarfile, os, re, math, randomï¼‰
"""

import os
import re
import math
import random
import urllib.request
import tarfile
from collections import defaultdict

DATASET_URL = "https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz"
DATASET_TAR = "aclImdb_v1.tar.gz"
DATA_DIR = "aclImdb"

# ----------------------------
# 1. ä¸‹è½½å¹¶è§£å‹ IMDb æ•°æ®é›†
# ----------------------------

def download_and_extract():
    if not os.path.exists(DATASET_TAR):
        print("æ­£åœ¨ä¸‹è½½ IMDb æ•°æ®é›†ï¼ˆçº¦ 80MBï¼‰...")
        urllib.request.urlretrieve(DATASET_URL, DATASET_TAR)
        print("âœ… ä¸‹è½½å®Œæˆ")

    if not os.path.exists(DATA_DIR):
        print("æ­£åœ¨è§£å‹æ•°æ®é›†...")
        with tarfile.open(DATASET_TAR, "r:gz") as tar:
            tar.extractall()
        print("âœ… è§£å‹å®Œæˆ")

# ----------------------------
# 2. åˆå¹¶è®­ç»ƒé›†ä¸ºçº¯æ–‡æœ¬æ–‡ä»¶
# ----------------------------

def create_txt_files():
    pos_file = "imdb_train_pos.txt"
    neg_file = "imdb_train_neg.txt"

    if os.path.exists(pos_file) and os.path.exists(neg_file):
        print("ğŸ“ çº¯æ–‡æœ¬è®­ç»ƒæ–‡ä»¶å·²å­˜åœ¨")
        return

    def read_reviews(folder_path):
        reviews = []
        for filename in os.listdir(folder_path):
            if filename.endswith(".txt"):
                with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:
                    reviews.append(f.read().strip())
        return reviews

    print("æ­£åœ¨åˆå¹¶æ­£é¢è¯„è®º...")
    pos_reviews = read_reviews(os.path.join(DATA_DIR, "train", "pos"))
    with open(pos_file, 'w', encoding='utf-8') as f:
        for r in pos_reviews:
            f.write(r.replace('\n', ' ') + '\n')

    print("æ­£åœ¨åˆå¹¶è´Ÿé¢è¯„è®º...")
    neg_reviews = read_reviews(os.path.join(DATA_DIR, "train", "neg"))
    with open(neg_file, 'w', encoding='utf-8') as f:
        for r in neg_reviews:
            f.write(r.replace('\n', ' ') + '\n')

    print(f"âœ… å·²ç”Ÿæˆ {len(pos_reviews)} æ¡æ­£é¢ + {len(neg_reviews)} æ¡è´Ÿé¢ è¯„è®º")

# ----------------------------
# 3. åŠ è½½çº¯æ–‡æœ¬æ•°æ®
# ----------------------------

def load_imdb_data():
    texts, labels = [], []

    with open("imdb_train_pos.txt", 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                texts.append(line)
                labels.append(1)

    with open("imdb_train_neg.txt", 'r', encoding='utf-8') as f:
        for line in f:
            line = line.strip()
            if line:
                texts.append(line)
                labels.append(0)

    return texts, labels

# ----------------------------
# 4. æ–‡æœ¬é¢„å¤„ç†
# ----------------------------

def preprocess(text):
    text = text.lower()
    text = re.sub(r'[^a-z\s]', ' ', text)  # åªä¿ç•™å­—æ¯å’Œç©ºæ ¼
    words = text.split()
    return [w for w in words if len(w) > 2]  # è¿‡æ»¤çŸ­è¯

# ----------------------------
# 5. æ‰‹åŠ¨ TF-IDF å‘é‡åŒ–
# ----------------------------

class SimpleTfidfVectorizer:
    def __init__(self, max_features=1000):
        self.max_features = max_features
        self.vocab = {}
        self.idf = {}

    def fit(self, texts):
        word_doc_freq = defaultdict(int)
        word_total_freq = defaultdict(int)

        for text in texts:
            words = preprocess(text)
            unique_words = set(words)
            for w in unique_words:
                word_doc_freq[w] += 1
            for w in words:
                word_total_freq[w] += 1

        sorted_vocab = sorted(word_total_freq.items(), key=lambda x: x[1], reverse=True)
        self.vocab = {word: idx for idx, (word, _) in enumerate(sorted_vocab[:self.max_features])}

        N = len(texts)
        for word in self.vocab:
            df = word_doc_freq[word]
            self.idf[word] = math.log(N / (df + 1)) + 1

    def transform(self, texts):
        vectors = []
        for text in texts:
            words = preprocess(text)
            if not words:
                vectors.append([0.0] * len(self.vocab))
                continue

            tf = defaultdict(int)
            for w in words:
                tf[w] += 1

            vec = [0.0] * len(self.vocab)
            for word, count in tf.items():
                if word in self.vocab:
                    idx = self.vocab[word]
                    tf_val = count / len(words)
                    vec[idx] = tf_val * self.idf[word]
            vectors.append(vec)
        return vectors

# ----------------------------
# 6. æ‰‹åŠ¨çº¿æ€§ SVM
# ----------------------------

class LinearSVM:
    def __init__(self, lr=0.01, epochs=500, reg=0.01):
        self.lr = lr
        self.epochs = epochs
        self.reg = reg
        self.w = None
        self.b = 0.0

    def fit(self, X, y):
        y = [1 if label == 1 else -1 for label in y]
        n_samples = len(X)
        n_features = len(X[0])
        self.w = [0.0] * n_features
        self.b = 0.0

        for _ in range(self.epochs):
            for i in range(n_samples):
                decision = sum(self.w[j] * X[i][j] for j in range(n_features)) + self.b
                if y[i] * decision < 1:
                    for j in range(n_features):
                        self.w[j] += self.lr * (y[i] * X[i][j] - self.reg * self.w[j])
                    self.b += self.lr * y[i]
                else:
                    for j in range(n_features):
                        self.w[j] -= self.lr * self.reg * self.w[j]

    def predict(self, X):
        preds = []
        for x in X:
            decision = sum(self.w[j] * x[j] for j in range(len(x))) + self.b
            preds.append(1 if decision >= 0 else 0)
        return preds

# ----------------------------
# 7. ä¸»ç¨‹åº
# ----------------------------

def main():
    # æ­¥éª¤1ï¼šä¸‹è½½å¹¶è§£å‹
    download_and_extract()

    # æ­¥éª¤2ï¼šç”Ÿæˆçº¯æ–‡æœ¬æ–‡ä»¶
    create_txt_files()

    # æ­¥éª¤3ï¼šåŠ è½½æ•°æ®
    texts, labels = load_imdb_data()
    print(f"æ€»è®­ç»ƒæ ·æœ¬æ•°: {len(texts)}")

    # éšæœºæ‰“ä¹±
    combined = list(zip(texts, labels))
    random.seed(42)
    random.shuffle(combined)
    texts, labels = zip(*combined)

    # åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•ï¼ˆ8:2ï¼‰
    split = int(0.8 * len(texts))
    X_train, X_test = texts[:split], texts[split:]
    y_train, y_test = labels[:split], labels[split:]

    print(f"è®­ç»ƒé›†: {len(X_train)} | æµ‹è¯•é›†: {len(X_test)}")

    # TF-IDF
    vectorizer = SimpleTfidfVectorizer(max_features=800)
    vectorizer.fit(X_train)
    X_train_vec = vectorizer.transform(X_train)
    X_test_vec = vectorizer.transform(X_test)

    # è®­ç»ƒ SVM
    print("æ­£åœ¨è®­ç»ƒ SVM æ¨¡å‹...")
    svm = LinearSVM(lr=0.01, epochs=600, reg=0.01)
    svm.fit(X_train_vec, y_train)

    # è¯„ä¼°
    y_pred = svm.predict(X_test_vec)
    accuracy = sum(a == b for a, b in zip(y_test, y_pred)) / len(y_test)
    print(f"\nğŸ¯ æµ‹è¯•å‡†ç¡®ç‡: {accuracy * 100:.2f}%")

    # é¢„æµ‹ç¤ºä¾‹
    def predict(text):
        vec = vectorizer.transform([text])[0]
        pred = svm.predict([vec])[0]
        print(f"\nè¾“å…¥: {text}\né¢„æµ‹: {'æ­£é¢' if pred == 1 else 'è´Ÿé¢'}")

    predict("This movie is fantastic and well directed!")
    predict("Boring, slow, and poorly acted.")

if __name__ == "__main__":
    main()